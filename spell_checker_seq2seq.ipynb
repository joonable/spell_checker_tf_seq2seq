{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "class SpellChecker():\n",
    "    def __init__(self, config):\n",
    "        self.lr = config['lr']\n",
    "        self.n_hidden= config['n_hidden']\n",
    "        self.total_epoch = config['total_epoch'] \n",
    "        self.batch_size = config['batch_size'] \n",
    "        self.char_arr = list(\"SEPabcdefghijklmnopqrstuvwxyz \")\n",
    "        self.n_class = self.n_input = self.dic_len = len(self.char_arr)\n",
    "        self.num_dic = {n: i for i, n in enumerate(self.char_arr)}\n",
    "        self.n_eval = config['n_eval']\n",
    "        \n",
    "        self.training_mode = True\n",
    "        self.output_keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "        # Checkpoint files will be saved in this directory during training\n",
    "        timestamp = str(int(time.time()))\n",
    "        self.checkpoint_dir = './checkpoints_' + timestamp + '/'\n",
    "        if os.path.exists(self.checkpoint_dir):\n",
    "            shutil.rmtree(self.checkpoint_dir)\n",
    "        os.makedirs(self.checkpoint_dir)\n",
    "        self.checkpoint_prefix = os.path.join(self.checkpoint_dir, 'model')        \n",
    "        \n",
    "\n",
    "        self.encoder_inputs= tf.placeholder(dtype = tf.float32, shape = [None, None, self.n_input], name = \"encoder_inputs\")\n",
    "        self.decoder_inputs = tf.placeholder(dtype = tf.float32, shape = [None, None, self.n_input], name = \"decoder_inputs\")\n",
    "        self.decoder_outputs = tf.placeholder(tf.int64, [None, None], name = \"decoder_outputs\")\n",
    "        self.target_weights = tf.placeholder(tf.float32, [None, None], name = \"target_weights\")\n",
    "\n",
    "        self.encoder_length = tf.placeholder(tf.int32, [None], name = \"encoder_length\")\n",
    "        self.decoder_length = tf.placeholder(tf.int32, [None], name = \"decoder_length\")\n",
    "        \n",
    "        # Embedding\n",
    "        # Look up embedding:\n",
    "        #   encoder_inputs: [max_time, batch_size]\n",
    "        #   encoder_emb_inp: [max_time, batch_size, embedding_size]\n",
    "        # self.embedding_size = 4\n",
    "\n",
    "        # self.embedding_encoder = tf.get_variable(\"embedding_encoder\", [self.dic_len, self.embedding_size])\n",
    "        # self.encoder_emb_inp = tf.nn.embedding_lookup(self.embedding_encoder, self.encoder_inputs)\n",
    "        # \n",
    "        # self.embedding_decoder = tf.get_variable(\"embedding_decoder\", [self.dic_len, self.embedding_size])\n",
    "        # self.decoder_emb_inp = tf.nn.embedding_lookup(self.embedding_decoder, self.decoder_inputs)\n",
    "        # self.decoder_emb_outp = tf.nn.embedding_lookup(self.embedding_decoder, self.decoder_outputs)\n",
    "         \n",
    "        \n",
    "        #[batch_size, time_steps, input_size]\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "        with tf.variable_scope('encode'):\n",
    "            self.enc_cell = tf.nn.rnn_cell.BasicLSTMCell(self.n_hidden)\n",
    "            self.enc_cell = tf.nn.rnn_cell.DropoutWrapper(self.enc_cell, output_keep_prob = self.output_keep_prob)\n",
    "            self.outputs, self.enc_states = tf.nn.dynamic_rnn(cell = self.enc_cell, inputs = self.encoder_inputs , dtype = tf.float32, sequence_length = self.encoder_length)\n",
    "    \n",
    "        with tf.variable_scope('decode'):\n",
    "            self.dec_cell = tf.nn.rnn_cell.BasicLSTMCell(self.n_hidden)\n",
    "            self.dec_cell = tf.nn.rnn_cell.DropoutWrapper(self.dec_cell, output_keep_prob = self.output_keep_prob)\n",
    "            self.projection_layer = tf.layers.Dense(self.dic_len, use_bias=True)\n",
    "            \n",
    "            self.helper = tf.contrib.seq2seq.TrainingHelper( self.decoder_inputs, self.decoder_length)\n",
    "            self.decoder = tf.contrib.seq2seq.BasicDecoder(self.dec_cell, self.helper, self.enc_states, output_layer = self.projection_layer)\n",
    "            \n",
    "            self.outputs, self.dec_states, self.final_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(self.decoder)\n",
    "\n",
    "        with tf.variable_scope('output'):\n",
    "            self.logits = self.outputs.rnn_output            \n",
    "            self.prediction = tf.argmax(self.logits, axis = 2)\n",
    "\n",
    "        with tf.variable_scope('cost'):\n",
    "            self.crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = self.logits, labels = self.decoder_outputs)\n",
    "            self.cost = (tf.reduce_mean(self.crossent * self.target_weights))\n",
    "            tf.summary.scalar('cost', self.cost )\n",
    "\n",
    "        with tf.variable_scope('accuracy_'):\n",
    "            correct_predictions = tf.equal(self.prediction, self.decoder_outputs)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name='accuracy')\n",
    "            tf.summary.scalar('accuracy', self.accuracy)\n",
    "        \n",
    "        # with tf.name_scope('num_correct'):\n",
    "        #     correct = tf.equal(self.prediction, tf.argmax(self.decoder_outputs, axis = 1))\n",
    "        #     self.num_correct = tf.reduce_sum(tf.cast(correct, 'float'))\n",
    "         \n",
    "        with tf.variable_scope('optimiser'):\n",
    "            self.params = tf.trainable_variables()\n",
    "            self.gradients = tf.gradients(self.cost, self.params)\n",
    "            self.clipped_gradients, _ = tf.clip_by_global_norm(self.gradients, 1)\n",
    "            self.opimiser= tf.train.AdamOptimizer(self.lr)         \n",
    "            self.train_op = self.opimiser.apply_gradients(zip(self.clipped_gradients, self.params), global_step = self.global_step)         \n",
    "            # self.train_op = tf.train.AdamOptimizer(self.lr).minimize(self.cost, global_step = self.global_step)\n",
    "            \n",
    "        self.df_train = pd.read_csv('./df_train.csv', index_col = False)\n",
    "        self.df_test = pd.read_csv('./df_test.csv', index_col = False)\n",
    "        self.df_train = self.df_train[['x', 'y']]\n",
    "        self.df_test = self.df_test[['x', 'y']]\n",
    "        \n",
    "        self.graph = tf.Graph()    \n",
    "        self.saver = tf.train.Saver()\n",
    "        self.sess = tf.Session()\n",
    "        self.train_writer = tf.summary.FileWriter('./train', self.sess.graph)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    \n",
    "    def batch_iter(self, data, batch_size, num_epochs):\n",
    "        data = np.array(data)\n",
    "        data_size = len(data)\n",
    "        num_batches_per_epoch = int(data_size / batch_size) + 1\n",
    "    \n",
    "        for epoch in range(num_epochs):\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "                yield data[start_index:end_index]\n",
    "                    \n",
    "                    \n",
    "    def make_batch(self, df): \n",
    "        enc_input_batch = []\n",
    "        dec_input_batch = []\n",
    "        dec_output_batch = []\n",
    "        target_weights_batch = []\n",
    "        \n",
    "        enc_len_batch = [] \n",
    "        dec_len_batch = []\n",
    "        \n",
    "        enc_max_len = 0\n",
    "        dec_max_len = 0\n",
    "        \n",
    "        #preprecessing\n",
    "        for i in range(0, len(df)):\n",
    "            if enc_max_len < len(df.loc[i, 'x']): enc_max_len = len(df.loc[i, 'x'])\n",
    "            if dec_max_len < len(df.loc[i, 'y']) + 1: dec_max_len = len(df.loc[i, 'y']) + 1\n",
    "            \n",
    "            enc_len_batch.append(len(df.loc[i, 'x']))\n",
    "            dec_len_batch.append(len(df.loc[i, 'y']) + 1)\n",
    "            \n",
    "        for i in range(0, len(df)):\n",
    "            input = [self.num_dic[n] for n in df.loc[i, 'x'].lower()]\n",
    "            output = [self.num_dic[n] for n in ('S' + df.loc[i, 'y'].lower())]\n",
    "            target = [self.num_dic[n] for n in (df.loc[i, 'y'].lower() + 'E')]\n",
    "            \n",
    "            target_weights_batch.extend([([1] * len(target)) + ([0] * (dec_max_len - len(target)))]) \n",
    "            \n",
    "            #pad sentence with 'P'\n",
    "            input = input + [2] * (enc_max_len - len(input))\n",
    "            output = output + [2] * (dec_max_len - len(output))\n",
    "            target = target + [2] * (dec_max_len - len(target))\n",
    "            \n",
    "            enc_input_batch.append(np.eye(self.dic_len)[input])\n",
    "            dec_input_batch.append(np.eye(self.dic_len)[output])\n",
    "            dec_output_batch.append(target)\n",
    "                        \n",
    "        # target_weights_batch = tf.squeeze(target_weights_batch)\n",
    "        return enc_input_batch, dec_input_batch, dec_output_batch, target_weights_batch, enc_len_batch, dec_len_batch\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        x_train= self.df_train.x.tolist()\n",
    "        y_train = self.df_train.y.tolist()\n",
    "        train_batches = self.batch_iter(data = list(zip(x_train, y_train)), batch_size = self.batch_size, num_epochs = self.total_epoch)\n",
    "        train_best_accuracy, val_best_accuracy, self.best_at_step = 0, 0, 0\n",
    "        \n",
    "        for train_batch in train_batches:\n",
    "            current_step = tf.train.global_step(self.sess, self.global_step)\n",
    "            train_enc_input_batch, train_dec_input_batch, train_dec_output_batch, train_target_weights_batch, train_enc_len_batch, train_dec_len_batch \\\n",
    "                = self.make_batch(pd.DataFrame(train_batch, columns = ['x','y']))\n",
    "            feed_dict = {\n",
    "                self.encoder_inputs: train_enc_input_batch,\n",
    "                self.decoder_inputs: train_dec_input_batch,\n",
    "                self.decoder_outputs: train_dec_output_batch,\n",
    "                self.target_weights: train_target_weights_batch,\n",
    "                self.encoder_length: train_enc_len_batch, \n",
    "                self.decoder_length: train_dec_len_batch,\n",
    "                self.output_keep_prob: 0.75\n",
    "            }\n",
    "            self.merged_summaries = tf.summary.merge_all()\n",
    "            _, loss, accuracy, summary = self.sess.run([self.train_op, self.cost, self.accuracy, self.merged_summaries], feed_dict = feed_dict)\n",
    "            self.train_writer.add_summary(summary = summary, global_step = current_step)\n",
    "            \n",
    "            print('current_step = ', '{}'.format(current_step), ', cost = ', '{:.6f}'.format(loss), ', accuracy = ', '{:.6f}'.format(accuracy))\n",
    "            \n",
    "            if current_step % self.n_eval == 0:\n",
    "                val_enc_input_batch, val_dec_input_batch, val_dec_output_batch, val_target_weights_batch, val_enc_len_batch, val_dec_len_batch \\\n",
    "                    = self.make_batch(pd.DataFrame(train_batch, columns = ['x','y']))\n",
    "                \n",
    "                val_feed_dict = {\n",
    "                    self.encoder_inputs: val_enc_input_batch,\n",
    "                    self.decoder_inputs: val_dec_input_batch,\n",
    "                    self.decoder_outputs: val_dec_output_batch,\n",
    "                    self.target_weights: val_target_weights_batch,\n",
    "                    self.encoder_length: val_enc_len_batch, \n",
    "                    self.decoder_length: val_dec_len_batch,\n",
    "                    self.output_keep_prob: 1\n",
    "                }\n",
    "\n",
    "                val_loss, val_accuracy = self.sess.run([self.cost, self.accuracy], feed_dict = val_feed_dict)\n",
    "                \n",
    "                print('current_step = ', '{}'.format(current_step), ', val_cost = ', '{:.6f}'.format(val_loss), ', val_accuracy = ', '{:.6f}'.format(val_accuracy))\n",
    "                \n",
    "                if accuracy > train_best_accuracy and val_accuracy > val_best_accuracy:\n",
    "                    train_best_accuracy, val_best_accuracy, self.best_at_step = accuracy, val_accuracy, current_step\n",
    "                    \n",
    "                    path = self.saver.save(self.sess, self.checkpoint_prefix, global_step=current_step)\n",
    "                    print('Saved model {} at step {}'.format(path, self.best_at_step))\n",
    "                    print('Best accuracy {} and {} at step {}'.format(train_best_accuracy, val_best_accuracy, self.best_at_step))\n",
    "                    \n",
    "                        \n",
    "    def test(self):\n",
    "        self.saver.restore(self.sess, self.checkpoint_prefix + '-' + str(self.best_at_step))\n",
    "\n",
    "        enc_input_batch, dec_input_batch, dec_output_batch, target_weights_batch, enc_len_batch, dec_len_batch = self.make_batch(self.df_train)\n",
    "        \n",
    "        feed_dict = {\n",
    "            self.encoder_inputs: enc_input_batch, \n",
    "            self.decoder_inputs: dec_input_batch, \n",
    "            self.decoder_outputs: dec_output_batch, \n",
    "            self.target_weights: target_weights_batch, \n",
    "            self.encoder_length: enc_len_batch,  \n",
    "            self.decoder_length: dec_len_batch,\n",
    "            self.output_keep_prob: 1\n",
    "        }\n",
    "        \n",
    "        results, loss, accuracy = self.sess.run([self.prediction, self.cost, self.accuracy], feed_dict = feed_dict)\n",
    "        print('cost = ', '{:.6f}'.format(loss), ', accuracy = ', '{:.6f}'.format(accuracy))\n",
    "\n",
    "        decoded = []\n",
    "        for result in results:\n",
    "            decoded.append([self.char_arr[i] for i in result]) \n",
    "        \n",
    "        self.translated = []\n",
    "        for result in decoded:\n",
    "            try:\n",
    "                end = result.index('E')\n",
    "                self.translated.append([''.join(result[:end])])\n",
    "            except:\n",
    "                self.translated.append([''.join(result)])\n",
    "        return self.translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['lr'] = 0.003\n",
    "config['n_hidden'] = 128\n",
    "config['total_epoch'] = 1\n",
    "config['batch_size'] = 256\n",
    "config['n_eval'] = 20\n",
    "spell_checker = SpellChecker(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_step =  0 , cost =  1.987067 , accuracy =  0.021140\ncurrent_step =  0 , val_cost =  1.970463 , val_accuracy =  0.068015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model ./checkpoints_1526095583/model-0 at step 0\nBest accuracy 0.021139705553650856 and 0.0680147036910057 at step 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_step =  1 , cost =  2.110509 , accuracy =  0.066162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_step =  2 , cost =  2.142275 , accuracy =  0.093018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_step =  3 , cost =  1.843219 , accuracy =  0.078776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_step =  4 , cost =  1.809897 , accuracy =  0.096788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_step =  5 , cost =  1.800838 , accuracy =  0.102022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_step =  6 , cost =  1.646435 , accuracy =  0.086589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_step =  7 , cost =  1.301142 , accuracy =  0.068274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_step =  8 , cost =  1.786712 , accuracy =  0.089384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_step =  9 , cost =  1.484435 , accuracy =  0.080181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_step =  10 , cost =  1.725180 , accuracy =  0.101792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_step =  11 , cost =  1.476494 , accuracy =  0.093339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_step =  12 , cost =  1.542916 , accuracy =  0.099609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_step =  13 , cost =  1.707119 , accuracy =  0.109707\nINFO:tensorflow:Restoring parameters from ./checkpoints_1526095583/model-0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost =  1.482517 , accuracy =  0.050428\n"
     ]
    }
   ],
   "source": [
    "spell_checker.train()\n",
    "test_result = spell_checker.test()\n",
    "pd.DataFrame(test_result).to_csv(\"./train_result.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'encode/rnn/while/Exit_3:0' shape=(?, 128) dtype=float32>, h=<tf.Tensor 'encode/rnn/while/Exit_4:0' shape=(?, 128) dtype=float32>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell_checker.enc_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
